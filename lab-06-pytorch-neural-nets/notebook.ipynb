{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gmum/natural-language-processing-classes/blob/master/lab-5-pytorch-neural-nets/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hGaeARTQ-F0d"
   },
   "source": [
    "# Lab 5 - Introduction to neural networks with PyTorch\n",
    "\n",
    "Introduction based on pytorch tutorials written by Robert Guthrie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_SuG0Hbk22U3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the list of useful links:\n",
    "- https://www.tensorflow.org/tutorials for this site read all in the 'BEGINNER' section\n",
    "- https://sourcedexter.com/tensorflow-text-classification-python/ try to classify senetences with BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch\n",
    "\n",
    "If you choose the Torch way follow the next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Nfiv01G22U6"
   },
   "source": [
    "Introduction to Torch's tensor library\n",
    "======================================\n",
    "\n",
    "All of deep learning is computations on tensors, which are\n",
    "generalizations of a matrix that can be indexed in more than 2\n",
    "dimensions. We will see exactly what this means in-depth later. First,\n",
    "lets look what we can do with tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eoq25ju09rw2"
   },
   "outputs": [],
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-mp9dpM022U-"
   },
   "source": [
    "Creating Tensors\n",
    "------------------\n",
    "\n",
    "Tensors can be created from Python lists with the torch.Tensor()\n",
    "function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErLZd-eo22U_"
   },
   "outputs": [],
   "source": [
    "# torch.tensor(data) creates a torch.Tensor object with the given data.\n",
    "V_data = [1., 2., 3.]\n",
    "V = torch.tensor(V_data)\n",
    "print(V)\n",
    "\n",
    "# Creates a matrix\n",
    "M_data = [[1., 2., 3.], [4., 5., 6]]\n",
    "M = torch.tensor(M_data)\n",
    "print(M)\n",
    "\n",
    "# Create a 3D tensor of size 2x2x2.\n",
    "T_data = [[[1., 2.], [3., 4.]],\n",
    "          [[5., 6.], [7., 8.]]]\n",
    "T = torch.tensor(T_data)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_ppobhy22VD"
   },
   "source": [
    "What is a 3D tensor anyway? Think about it like this. If you have a\n",
    "vector, indexing into the vector gives you a scalar. If you have a\n",
    "matrix, indexing into the matrix gives you a vector. If you have a 3D\n",
    "tensor, then indexing into the tensor gives you a matrix!\n",
    "\n",
    "A note on terminology:\n",
    "when I say \"tensor\" in this tutorial, it refers\n",
    "to any torch.Tensor object. Matrices and vectors are special cases of\n",
    "torch.Tensors, where their dimension is 1 and 2 respectively. When I am\n",
    "talking about 3D tensors, I will explicitly use the term \"3D tensor\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jIqTF7Qv22VE"
   },
   "outputs": [],
   "source": [
    "# Index into V and get a scalar (0 dimensional tensor)\n",
    "print(V[0])\n",
    "# Get a Python number from it\n",
    "print(V[0].item())\n",
    "\n",
    "# Index into M and get a vector\n",
    "print(M[0])\n",
    "\n",
    "# Index into T and get a matrix\n",
    "print(T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OAoToN_C22VH"
   },
   "source": [
    "You can also create tensors of other datatypes. The default, as you can\n",
    "see, is Float. To create a tensor of integer types, try\n",
    "torch.LongTensor(). Check the documentation for more data types, but\n",
    "Float and Long will be the most common.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kvIc4Et-22VJ"
   },
   "source": [
    "You can create a tensor with random data and the supplied dimensionality\n",
    "with torch.randn()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JP2jyU1t22VJ"
   },
   "outputs": [],
   "source": [
    "x = torch.randn((3, 4, 5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Pb5XhFn22VN"
   },
   "source": [
    "Operations with Tensors\n",
    "--------------------------\n",
    "\n",
    "You can operate on tensors in the ways you would expect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6CC6ZNJp22VN"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor([4., 5., 6.])\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CkA_777c22VR"
   },
   "source": [
    "See `the documentation <http://pytorch.org/docs/torch.html>`__ for a\n",
    "complete list of the massive number of operations available to you. They\n",
    "expand beyond just mathematical operations.\n",
    "\n",
    "One helpful operation that we will make use of later is concatenation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OpUTdyZ22VT"
   },
   "outputs": [],
   "source": [
    "# By default, it concatenates along the first axis (concatenates rows)\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "print(z_1)\n",
    "\n",
    "# Concatenate columns:\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "# second arg specifies which axis to concat along\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2)\n",
    "\n",
    "# If your tensors are not compatible, torch will complain.  Uncomment to see the error\n",
    "# torch.cat([x_1, x_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEXk41vc22VX"
   },
   "source": [
    "Reshaping Tensors\n",
    "--------------------\n",
    "\n",
    "Use the .view() method to reshape a tensor. This method receives heavy\n",
    "use, because many neural network components expect their inputs to have\n",
    "a certain shape. Often you will need to reshape before passing your data\n",
    "to the component.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sKDUwpbP22VZ"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12))  # Reshape to 2 rows, 12 columns\n",
    "# Same as above.  If one of the dimensions is -1, its size can be inferred\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2MRgj7O-22Vc"
   },
   "source": [
    "Computation Graphs and Automatic Differentiation\n",
    "================================================\n",
    "\n",
    "The concept of a computation graph is essential to efficient deep\n",
    "learning programming, because it allows you to not have to write the\n",
    "back propagation gradients yourself. A computation graph is simply a\n",
    "specification of how your data is combined to give you the output. Since\n",
    "the graph totally specifies what parameters were involved with which\n",
    "operations, it contains enough information to compute derivatives. This\n",
    "probably sounds vague, so let's see what is going on using the\n",
    "fundamental flag ``requires_grad``.\n",
    "\n",
    "First, think from a programmers perspective. What is stored in the\n",
    "torch.Tensor objects we were creating above? Obviously the data and the\n",
    "shape, and maybe a few other things. But when we added two tensors\n",
    "together, we got an output tensor. All this output tensor knows is its\n",
    "data and shape. It has no idea that it was the sum of two other tensors\n",
    "(it could have been read in from a file, it could be the result of some\n",
    "other operation, etc.)\n",
    "\n",
    "If ``requires_grad=True``, the Tensor object keeps track of how it was\n",
    "created. Lets see it in action.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DdfXrpMJ22Ve"
   },
   "outputs": [],
   "source": [
    "# Tensor factory methods have a ``requires_grad`` flag\n",
    "x = torch.tensor([1., 2., 3], requires_grad=True)\n",
    "\n",
    "# With requires_grad=True, you can still do all the operations you previously\n",
    "# could\n",
    "y = torch.tensor([4., 5., 6], requires_grad=True)\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "# BUT z knows something extra.\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWFaN-g622Vh"
   },
   "source": [
    "So Tensors know what created them. z knows that it wasn't read in from\n",
    "a file, it wasn't the result of a multiplication or exponential or\n",
    "whatever. And if you keep following z.grad_fn, you will find yourself at\n",
    "x and y.\n",
    "\n",
    "But how does that help us compute a gradient?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pOuwCLK22Vi"
   },
   "outputs": [],
   "source": [
    "# Lets sum up all the entries in z\n",
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ccr3Tce622Vm"
   },
   "source": [
    "So now, what is the derivative of this sum with respect to the first\n",
    "component of x? In math, we want\n",
    "\n",
    "\\begin{align}\\frac{\\partial s}{\\partial x_0}\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "Well, s knows that it was created as a sum of the tensor z. z knows\n",
    "that it was the sum x + y. So\n",
    "\n",
    "\\begin{align}s = \\overbrace{x_0 + y_0}^\\text{$z_0$} + \\overbrace{x_1 + y_1}^\\text{$z_1$} + \\overbrace{x_2 + y_2}^\\text{$z_2$}\\end{align}\n",
    "\n",
    "And so s contains enough information to determine that the derivative\n",
    "we want is 1:\n",
    "\n",
    "\\begin{align}\\frac{\\partial s}{\\partial x_0} = \\overbrace{\\frac{\\partial x_0}{\\partial x_0} + 0}^\\text{$z_0$} + \\overbrace{0 + 0}^\\text{$z_1$} + \\overbrace{0 + 0}^\\text{$z_2$} = 1\\end{align}\n",
    "\n",
    "\n",
    "Of course this glosses over the challenge of how to actually compute\n",
    "that derivative. The point here is that s is carrying along enough\n",
    "information that it is possible to compute it. In reality, the\n",
    "developers of Pytorch program the sum() and + operations to know how to\n",
    "compute their gradients, and run the back propagation algorithm. An\n",
    "in-depth discussion of that algorithm is beyond the scope of this\n",
    "tutorial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g37vpDQj22Vo"
   },
   "source": [
    "Lets have Pytorch compute the gradient, and see that we were right:\n",
    "(note if you run this block multiple times, the gradient will increment.\n",
    "That is because Pytorch *accumulates* the gradient into the .grad\n",
    "property, since for many models this is very convenient.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3hQwYucm22Vo"
   },
   "outputs": [],
   "source": [
    "# calling .backward() on any variable will run backprop, starting from it.\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOhzqYpo22Vs"
   },
   "source": [
    "Understanding what is going on in the block below is crucial for being a\n",
    "successful programmer in deep learning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XYKFycwj22Vv"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(2, 2)\n",
    "y = torch.randn(2, 2)\n",
    "# By default, user created Tensors have ``requires_grad=False``\n",
    "print(x.requires_grad, y.requires_grad)\n",
    "z = x + y\n",
    "# So you can't backprop through z\n",
    "print(z.grad_fn)\n",
    "print(z.requires_grad)\n",
    "print()\n",
    "\n",
    "# ``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n",
    "# flag in-place. The input flag defaults to ``True`` if not given.\n",
    "x = x.requires_grad_()\n",
    "y = y.requires_grad_()\n",
    "# z contains enough information to compute gradients, as we saw above\n",
    "z = x + y\n",
    "print(z.grad_fn)\n",
    "# If any input to an operation has ``requires_grad=True``, so will the output\n",
    "print(z.requires_grad)\n",
    "print()\n",
    "\n",
    "# Now z has the computation history that relates itself to x and y\n",
    "# Can we just take its values, and **detach** it from its history?\n",
    "new_z = z.detach()\n",
    "\n",
    "# ... does new_z have information to backprop to x and y?\n",
    "# NO!\n",
    "print(new_z.grad_fn)\n",
    "print(new_z.requires_grad)\n",
    "# And how could it? ``z.detach()`` returns a tensor that shares the same storage\n",
    "# as ``z``, but with the computation history forgotten. It doesn't know anything\n",
    "# about how it was computed.\n",
    "# In essence, we have broken the Tensor away from its past history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rOgbHrjF22Vy"
   },
   "source": [
    "You can also stop autograd from tracking history on Tensors\n",
    "with ``.requires_grad``=True by wrapping the code block in\n",
    "``with torch.no_grad():``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFFbDsPj22Vz"
   },
   "outputs": [],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "\tprint((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_Sd5HJDHiny"
   },
   "source": [
    "Deep Learning Building Blocks: Affine maps, non-linearities and objectives\n",
    "==========================================================================\n",
    "\n",
    "Deep learning consists of composing linearities with non-linearities in\n",
    "clever ways. The introduction of non-linearities allows for powerful\n",
    "models. In this section, we will play with these core components, make\n",
    "up an objective function, and see how the model is trained.\n",
    "\n",
    "\n",
    "Affine Maps\n",
    "-------------\n",
    "\n",
    "One of the core workhorses of deep learning is the affine map, which is\n",
    "a function $f(x)$ where\n",
    "\n",
    "\\begin{align}f(x) = Ax + b\\end{align}\n",
    "\n",
    "for a matrix $A$ and vectors $x, b$. The parameters to be\n",
    "learned here are $A$ and $b$. Often, $b$ is refered to\n",
    "as the *bias* term.\n",
    "\n",
    "\n",
    "PyTorch and most other deep learning frameworks do things a little\n",
    "differently than traditional linear algebra. It maps the rows of the\n",
    "input instead of the columns. That is, the $i$'th row of the\n",
    "output below is the mapping of the $i$'th row of the input under\n",
    "$A$, plus the bias term. Look at the example below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r5wF9WzbJ4LR"
   },
   "outputs": [],
   "source": [
    "lin = nn.Linear(5, 3)  # maps from R^5 to R^3, parameters A, b\n",
    "# data is 2x5.  A maps from 5 to 3... can we map \"data\" under A?\n",
    "data = torch.randn(2, 5)\n",
    "print(lin(data))  # yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GjFDJnWHKexy"
   },
   "source": [
    "Non-Linearities\n",
    "--\n",
    "\n",
    "First, note the following fact, which will explain why we need\n",
    "non-linearities in the first place. Suppose we have two affine maps\n",
    "$f(x) = Ax + b$ and $g(x) = Cx + d$. What is\n",
    "$f(g(x))$?\n",
    "\n",
    "\\begin{align}f(g(x)) = A(Cx + d) + b = ACx + (Ad + b)\\end{align}\n",
    "\n",
    "$AC$ is a matrix and $Ad + b$ is a vector, so we see that\n",
    "composing affine maps gives you an affine map.\n",
    "\n",
    "From this, you can see that if you wanted your neural network to be long\n",
    "chains of affine compositions, that this adds no new power to your model\n",
    "than just doing a single affine map.\n",
    "\n",
    "If we introduce non-linearities in between the affine layers, this is no\n",
    "longer the case, and we can build much more powerful models.\n",
    "\n",
    "There are a few core non-linearities.\n",
    "$\\tanh(x), \\sigma(x), \\text{ReLU}(x)$ are the most common. You are\n",
    "probably wondering: \"why these functions? I can think of plenty of other\n",
    "non-linearities.\" The reason for this is that they have gradients that\n",
    "are easy to compute, and computing gradients is essential for learning.\n",
    "For example\n",
    "\n",
    "\\begin{align}\\frac{d\\sigma}{dx} = \\sigma(x)(1 - \\sigma(x))\\end{align}\n",
    "\n",
    "A quick note: although you may have learned some neural networks in your\n",
    "intro to AI class where $\\sigma(x)$ was the default non-linearity,\n",
    "typically people shy away from it in practice. This is because the\n",
    "gradient *vanishes* very quickly as the absolute value of the argument\n",
    "grows. Small gradients means it is hard to learn. Most people default to\n",
    "tanh or ReLU.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ehv5akHOJ4oc"
   },
   "outputs": [],
   "source": [
    "# In pytorch, most non-linearities are in torch.functional (we have it imported as F)\n",
    "# Note that non-linearites typically don't have parameters like affine maps do.\n",
    "# That is, they don't have weights that are updated during training.\n",
    "data = torch.randn(2, 2)\n",
    "print(data)\n",
    "print(f'relu: {F.relu(data)}')\n",
    "print(f'tanh: {torch.tanh(data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i0ktpUv9MTmR"
   },
   "source": [
    "Softmax and Probabilities\n",
    "---\n",
    "\n",
    "The function $\\text{Softmax}(x)$ is also just a non-linearity, but\n",
    "it is special in that it usually is the last operation done in a\n",
    "network. This is because it takes in a vector of real numbers and\n",
    "returns a probability distribution. Its definition is as follows. Let\n",
    "$x$ be a vector of real numbers (positive, negative, whatever,\n",
    "there are no constraints). Then the i'th component of\n",
    "$\\text{Softmax}(x)$ is\n",
    "\n",
    "\\begin{align}\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\end{align}\n",
    "\n",
    "It should be clear that the output is a probability distribution: each\n",
    "element is non-negative and the sum over all components is 1.\n",
    "\n",
    "You could also think of it as just applying an element-wise\n",
    "exponentiation operator to the input to make everything non-negative and\n",
    "then dividing by the normalization constant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u62UvZQJNlNB"
   },
   "outputs": [],
   "source": [
    "# Softmax is also in torch.nn.functional\n",
    "data = torch.randn(5)\n",
    "print(data)\n",
    "print(F.softmax(data, dim=0))\n",
    "print(F.softmax(data, dim=0).sum())  # Sums to 1 because it is a distribution!\n",
    "print(F.log_softmax(data, dim=0))  # theres also log_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fbYaIOFPNnyY"
   },
   "source": [
    "Objective Functions (Loss Functions/Cost Functions)\n",
    "--\n",
    "\n",
    "The objective function is the function that your network is being\n",
    "trained to minimize (in which case it is often called a *loss function*\n",
    "or *cost function*). This proceeds by first choosing a training\n",
    "instance, running it through your neural network, and then computing the\n",
    "loss of the output. The parameters of the model are then updated by\n",
    "taking the derivative of the loss function. Intuitively, if your model\n",
    "is completely confident in its answer, and its answer is wrong, your\n",
    "loss will be high. If it is very confident in its answer, and its answer\n",
    "is correct, the loss will be low.\n",
    "\n",
    "The idea behind minimizing the loss function on your training examples\n",
    "is that your network will hopefully generalize well and have small loss\n",
    "on unseen examples in your dev set, test set, or in production. An\n",
    "example loss function is the *negative log likelihood loss*, which is a\n",
    "very common objective for multi-class classification. For supervised\n",
    "multi-class classification, this means training the network to minimize\n",
    "the negative log probability of the correct output (or equivalently,\n",
    "maximize the log probability of the correct output).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOnKRrkWPE9I"
   },
   "source": [
    "Optimization and Training\n",
    "=========================\n",
    "\n",
    "So what we can compute a loss function for an instance? What do we do\n",
    "with that? We saw earlier that Tensors know how to compute gradients\n",
    "with respect to the things that were used to compute it. Well,\n",
    "since our loss is an Tensor, we can compute gradients with\n",
    "respect to all of the parameters used to compute it! Then we can perform\n",
    "standard gradient updates. Let $\\theta$ be our parameters,\n",
    "$L(\\theta)$ the loss function, and $\\eta$ a positive\n",
    "learning rate. Then:\n",
    "\n",
    "\\begin{align}\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta)\\end{align}\n",
    "\n",
    "There are a huge collection of algorithms and active research in\n",
    "attempting to do something more than just this vanilla gradient update.\n",
    "Many attempt to vary the learning rate based on what is happening at\n",
    "train time. Torch provides\n",
    "many in the torch.optim package, and they are all completely\n",
    "transparent. Using the simplest gradient update is the same as the more\n",
    "complicated algorithms. Trying different update algorithms and different\n",
    "parameters for the update algorithms (like different initial learning\n",
    "rates) is important in optimizing your network's performance. Often,\n",
    "just replacing vanilla SGD with an optimizer like Adam or RMSProp will\n",
    "boost performance noticably.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JVvyLn-dPFAV"
   },
   "source": [
    "Creating Network Components in PyTorch\n",
    "======================================\n",
    "\n",
    "Before we move on to our focus on NLP, lets do an annotated example of\n",
    "building a network in PyTorch using only affine maps and\n",
    "non-linearities. We will also see how to compute a loss function, using\n",
    "PyTorch's built in negative log likelihood, and update parameters by\n",
    "backpropagation.\n",
    "\n",
    "All network components should inherit from nn.Module and override the\n",
    "forward() method. That is about it, as far as the boilerplate is\n",
    "concerned. Inheriting from nn.Module provides functionality to your\n",
    "component. For example, it makes it keep track of its trainable\n",
    "parameters, you can swap it between CPU and GPU with the ``.to(device)``\n",
    "method, where device can be a CPU device ``torch.device(\"cpu\")`` or CUDA\n",
    "device ``torch.device(\"cuda:0\")``.\n",
    "\n",
    "Let's write an annotated example of a network that takes in a sparse\n",
    "bag-of-words representation and outputs a probability distribution over\n",
    "two labels: \"English\" and \"Spanish\". This model is just logistic\n",
    "regression.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EsU1kBJGPyKi"
   },
   "source": [
    "Example: Logistic Regression Bag-of-Words classifier\n",
    "--\n",
    "\n",
    "Our model will map a sparse BoW representation to log probabilities over\n",
    "labels. We assign each word in the vocab an index. For example, say our\n",
    "entire vocab is two words \"hello\" and \"world\", with indices 0 and 1\n",
    "respectively. The BoW vector for the sentence \"hello hello hello hello\"\n",
    "is\n",
    "\n",
    "\\begin{align}\\left[ 4, 0 \\right]\\end{align}\n",
    "\n",
    "For \"hello world world hello\", it is\n",
    "\n",
    "\\begin{align}\\left[ 2, 2 \\right]\\end{align}\n",
    "\n",
    "etc. In general, it is\n",
    "\n",
    "\\begin{align}\\left[ \\text{Count}(\\text{hello}), \\text{Count}(\\text{world}) \\right]\\end{align}\n",
    "\n",
    "Denote this BOW vector as $x$. The output of our network is:\n",
    "\n",
    "\\begin{align}\\log \\text{Softmax}(Ax + b)\\end{align}\n",
    "\n",
    "That is, we pass the input through an affine map and then do log\n",
    "softmax.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vsfh2ESwPBYL"
   },
   "outputs": [],
   "source": [
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
    "# index into the Bag of words vector\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2\n",
    "\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Make sure you understand why the input dimension is vocab_size\n",
    "        # and the output is num_labels!\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
    "        # to worry about that here\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)\n",
    "\n",
    "\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])\n",
    "\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "\n",
    "# the model knows its parameters.  The first output below is A, the second is b.\n",
    "# Whenever you assign a component to a class variable in the __init__ function\n",
    "# of a module, which was done with the line\n",
    "# self.linear = nn.Linear(...)\n",
    "# Then through some Python magic from the PyTorch devs, your module\n",
    "# (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters\n",
    "for param in model.parameters():\n",
    "    print(f'param: {param}')\n",
    "\n",
    "# To run the model, pass in a BoW vector\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    sample = test_data[0]\n",
    "    bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "    # As we have inherited from nn.Module, we can just use model(vector) instead of model.forward(vector)\n",
    "    log_probs = model(bow_vector)\n",
    "    print(f'logprobs: {log_probs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TikUcelhViP1"
   },
   "source": [
    "Which of the above values corresponds to the log probability of ENGLISH,\n",
    "and which to SPANISH? We never defined it, but we need to if we want to\n",
    "train the thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhtN6pCGVjsP"
   },
   "outputs": [],
   "source": [
    "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tNQsMgEbVmEk"
   },
   "source": [
    "So lets train! To do this, we pass instances through to get log\n",
    "probabilities, compute a loss function, compute the gradient of the loss\n",
    "function, and then update the parameters with a gradient step. Loss\n",
    "functions are provided by Torch in the nn package. nn.NLLLoss() is the\n",
    "negative log likelihood loss we want. It also defines optimization\n",
    "functions in torch.optim. Here, we will just use SGD.\n",
    "\n",
    "Note that the *input* to NLLLoss is a vector of log probabilities, and a\n",
    "target label. It doesn't compute the log probabilities for us. This is\n",
    "why the last layer of our network is log softmax. The loss function\n",
    "nn.CrossEntropyLoss() is the same as NLLLoss(), except it does the log\n",
    "softmax for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YyZGiZPIWBZg"
   },
   "outputs": [],
   "source": [
    "# Run on test data before we train, just to see a before-and-after\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(log_probs)\n",
    "\n",
    "# Print the matrix column corresponding to \"creo\"\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Usually you want to pass over the training data several times.\n",
    "# 100 is much bigger than on a real data set, but real datasets have more than\n",
    "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        # Step 1. Remember that PyTorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Make our BOW vector and also we must wrap the target in a\n",
    "        # Tensor as an integer. For example, if the target is SPANISH, then\n",
    "        # we wrap the integer 0. The loss function then knows that the 0th\n",
    "        # element of the log probabilities is the log probability\n",
    "        # corresponding to SPANISH\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        target = make_target(label, label_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        log_probs = model(bow_vec)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(log_probs)\n",
    "\n",
    "# Index corresponding to Spanish goes up, English goes down!\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jtrs-MpoWCtF"
   },
   "source": [
    "We got the right answer! You can see that the log probability for\n",
    "Spanish is much higher in the first example, and the log probability for\n",
    "English is much higher in the second for the test data, as it should be.\n",
    "\n",
    "Now you see how to make a PyTorch component, pass some data through it\n",
    "and do gradient updates. We are ready to dig deeper into what deep NLP\n",
    "has to offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6xqKOWq0LNSV"
   },
   "source": [
    "# Excercise 1. (2pt)\n",
    "Read above tutorial and do the following:\n",
    "\n",
    "1) Write down the formulas for tanh and ReLU functions, and for their derivatives.\n",
    "\n",
    "2) Base on above example to train a neural network that classifies sentences into classes {'hp', 'lotr'}. Write code that evaluates accuracy on train and test set after every epoch. Obtain **75%** accuracy. \n",
    "\n",
    "hints:\n",
    "- To prepare train/test datasets you can base on this code:\n",
    "\n",
    "```\n",
    "lotr_data = [(normalize(word_tokenize(el)), 'lotr') for el in sent_tokenize(lotr)]\n",
    "data = lotr_data + hp_data\n",
    "test_data = test_lotr + test_hp\n",
    "```\n",
    "- try to use preprocessing to make the data more suitable for training. (lab-2)\n",
    "\n",
    "- optionally, you can modify the neural network architecture a little, for example (playing with preprocessing only should give 75%):\n",
    "  - you can modify the number of training epochs,\n",
    "  - you can modify the optimizer (i.e. Adam optimizer) and learning rate,\n",
    "  - you can use batches instead of learning one example at a time,\n",
    "  - you can add hidden layer(s), activation functions (like relu) etc. but be careful with overfitting! dropout or other regularization might help to mitigate overfitting,\n",
    "  - eventually, you can play with the loss function, parameter initialization\n",
    " \n",
    "\n",
    "Due to small dataset size (for a neural network) and the specificity of the task, it is harder to obtain higher accuracy than Naive Bayes, and some of above tricks might not work well. Don't worry - soon we will learn what neural nets are capable of!\n",
    "\n",
    "Use the data located in https://drive.google.com/open?id=1xLS_gxqGidnFk24as498_SWNV4xF79JI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7gZzjuz2_ylB"
   },
   "outputs": [],
   "source": [
    "#  This time we will load the texts from google drive, to save some space in the notebook :)\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ZkQOru3AQhK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Download the folder to your drive and paste the path to your folder below\n",
    "path_to_folder = '/content/gdrive/My Drive/nlp-classes/labs/lab-5/' # this should be path to the folder in your drive\n",
    "\n",
    "with open(os.path.join(path_to_folder, 'hp.txt'), 'r') as f:\n",
    "  hp = f.read().replace('\\n', ' ')\n",
    "with open(os.path.join(path_to_folder, 'lotr.txt'), 'r') as f:\n",
    "  lotr = f.read().replace('\\n', ' ')\n",
    "with open(os.path.join(path_to_folder, 'hp_test.txt'), 'r') as f:\n",
    "  test_hp = f.read().replace('\\n', ' ')\n",
    "with open(os.path.join(path_to_folder, 'lotr_test.txt'), 'r') as f:\n",
    "  test_lotr = f.read().replace('\\n', ' ')\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "lab-5-pytorch-neural-nets.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
